{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Ass_3_Khaled_Fayed_NoteBook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNZq2XmypsYVVy72IgPDXqk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KhaledGhaleb/CorelDataSet/blob/main/Ass_3_Khaled_Fayed_NoteBook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hc5MTwGHgjPG"
      },
      "source": [
        "# **Assignment 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72jZl8q6mZii"
      },
      "source": [
        "Get Corel Reduced Data from Github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYd4LO3kmop9"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "_URL = 'https://raw.githubusercontent.com/KhaledGhaleb/CorelDataSet/main/CORELREduced.zip'\n",
        "path_to_zip = tf.keras.utils.get_file('COREL-REduced.zip', origin=_URL, extract=True)\n",
        "PATH = os.path.join(os.path.dirname(path_to_zip), 'COREL-REduced')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35A6vQtMmCMc"
      },
      "source": [
        "# 1- Pretrained VGG16 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV2ZX4rnygil"
      },
      "source": [
        "Prepare Train and Validation data:\n",
        "\n",
        "set ratio of 0.5 validation = 20 * 0.5 = 10 validation\n",
        "\n",
        "Set Image size to 224*224 * 3 to support VGG16\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uBTtluWyfnR",
        "outputId": "723f166f-50a5-4330-eda9-1faf8e5f8bd8"
      },
      "source": [
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "BATCH_SIZE = 128\n",
        "IMG_SIZE = (224, 224)\n",
        "COLOR_MODE = 'rgb' #\"grayscale\"  #  \"rgb\"\n",
        "Color_Num = 3\n",
        "Valid_ratio = 0.5\n",
        "\n",
        "train_dataset_VGG16 = image_dataset_from_directory(PATH,\n",
        "                                             seed = 1234,                                           \n",
        "                                             color_mode = COLOR_MODE, \n",
        "                                             validation_split = Valid_ratio,\n",
        "                                             subset = \"training\",\n",
        "                                             image_size=IMG_SIZE,\n",
        "                                             batch_size=BATCH_SIZE,\n",
        "                                             label_mode ='categorical',\n",
        "                                             )\n",
        "Validation_dataset_VGG16 = image_dataset_from_directory(PATH,\n",
        "                                             seed = 1234,                                           \n",
        "                                             color_mode = COLOR_MODE, \n",
        "                                             validation_split = Valid_ratio,\n",
        "                                             subset = 'validation',\n",
        "                                             image_size=IMG_SIZE,\n",
        "                                             batch_size=BATCH_SIZE,\n",
        "                                             label_mode ='categorical',\n",
        "                                             )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 80 files belonging to 4 classes.\n",
            "Using 40 files for training.\n",
            "Found 80 files belonging to 4 classes.\n",
            "Using 40 files for validation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPBRTUcHxICN"
      },
      "source": [
        "Normalize data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMtJYBq7xICS"
      },
      "source": [
        "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
        "#reshape_layer = tf.keras.layers.Reshape(3, 4)\n",
        "train_normalized_ds_VGG16 = train_dataset_VGG16.map(lambda x, y: (normalization_layer(x), y))\n",
        "validation_normalized_ds_VGG16 = Validation_dataset_VGG16.map(lambda x, y: (normalization_layer(x), y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GlzDwLbxICT"
      },
      "source": [
        "Convert normalized dataset to images and labels in nparray "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVtFPXKVxICU"
      },
      "source": [
        "train_images_VGG16,train_labels_VGG16  = list(train_normalized_ds_VGG16.as_numpy_iterator())[0]\n",
        "validate_images_VGG16,validate_labels_VGG16  = list(validation_normalized_ds_VGG16.as_numpy_iterator())[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YO-vn6sSkZK4"
      },
      "source": [
        "#tf.keras.layers.experimental.preprocessing.Resizing( 293, 293, interpolation=\"bilinear\", crop_to_aspect_ratio=False)\n",
        "#train_images_GoogleNet = train_images.resize(293,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySzffFj9gn51"
      },
      "source": [
        "Generate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVogAKMFgW-z"
      },
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "model_VGG16 = VGG16(weights='imagenet') #,include_top=False\n",
        "#model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PYj3qPb4Pix"
      },
      "source": [
        "Create Feautre Arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OLTMmLGXwXy"
      },
      "source": [
        "TrainFeaturesArr_VGG16 = model_VGG16.predict(train_images_VGG16)\n",
        "ValidateFeaturesArr_VGG16 = model_VGG16.predict(validate_images_VGG16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0sqrdGwc4kM"
      },
      "source": [
        "Add KNN layer with K = 3 and P = 1 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tWTwBCUbIcy"
      },
      "source": [
        "from sklearn import neighbors\n",
        "knn_VGG16 = neighbors.KNeighborsClassifier(n_neighbors = 3, p = 1)\n",
        "knn_VGG16.fit(TrainFeaturesArr_VGG16, train_labels_VGG16)\n",
        "y_pred_VGG16 = knn_VGG16.predict(ValidateFeaturesArr_VGG16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWfIZXZfbqzt",
        "outputId": "5fccc545-b50f-4066-a286-598fda83212d"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "print('Accuracy score: ',accuracy_score(y_pred_VGG16, validate_labels_VGG16))\n",
        "print('F1 Score: ',f1_score(y_pred_VGG16, validate_labels_VGG16, average=\"macro\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score:  0.825\n",
            "F1 Score:  0.845711500974659\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AltmnzSVgrkn"
      },
      "source": [
        "# 2- Pretrained GoogleNet (InceptionV3)\n",
        "using InceptionV3 pretrained with imagenet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IuFTA7uoGGL"
      },
      "source": [
        "Prepare Train and Validation data:\n",
        "\n",
        "set ratio of 0.5 validation = 20 * 0.5 = 10 validation\n",
        "\n",
        "Set Image size to 299*299 * 3 to support InceptionV3\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ghk0Gq9EoGGc",
        "outputId": "5f533711-1647-4988-dbd9-c8a05dde699e"
      },
      "source": [
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "BATCH_SIZE = 128\n",
        "IMG_SIZE = (299, 299)\n",
        "COLOR_MODE = 'rgb' #\"grayscale\"  #  \"rgb\"\n",
        "Color_Num = 3\n",
        "Valid_ratio = 0.5\n",
        "\n",
        "train_dataset_InceptionV3 = image_dataset_from_directory(PATH,\n",
        "                                             seed = 1234,                                           \n",
        "                                             color_mode = COLOR_MODE, \n",
        "                                             validation_split = Valid_ratio,\n",
        "                                             subset = \"training\",\n",
        "                                             image_size=IMG_SIZE,\n",
        "                                             batch_size=BATCH_SIZE,\n",
        "                                             label_mode ='categorical',\n",
        "                                             )\n",
        "Validation_dataset_InceptionV3 = image_dataset_from_directory(PATH,\n",
        "                                             seed = 1234,                                           \n",
        "                                             color_mode = COLOR_MODE, \n",
        "                                             validation_split = Valid_ratio,\n",
        "                                             subset = 'validation',\n",
        "                                             image_size=IMG_SIZE,\n",
        "                                             batch_size=BATCH_SIZE,\n",
        "                                             label_mode ='categorical',\n",
        "                                             )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 80 files belonging to 4 classes.\n",
            "Using 40 files for training.\n",
            "Found 80 files belonging to 4 classes.\n",
            "Using 40 files for validation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Do_6zmqoGGe"
      },
      "source": [
        "Normalize data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQ7ZaeKQoGGe"
      },
      "source": [
        "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
        "#reshape_layer = tf.keras.layers.Reshape(3, 4)\n",
        "train_normalized_ds_InceptionV3 = train_dataset_InceptionV3.map(lambda x, y: (normalization_layer(x), y))\n",
        "validation_normalized_ds_InceptionV3 = Validation_dataset_InceptionV3.map(lambda x, y: (normalization_layer(x), y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n5xUaFNoGGj"
      },
      "source": [
        "Convert normalized dataset to images and labels in nparray "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6OhI00soGGk"
      },
      "source": [
        "train_images_InceptionV3,train_labels_InceptionV3  = list(train_normalized_ds_InceptionV3.as_numpy_iterator())[0]\n",
        "validate_images_InceptionV3,validate_labels_InceptionV3  = list(validation_normalized_ds_InceptionV3.as_numpy_iterator())[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H0QgkG1oGGl"
      },
      "source": [
        "Generate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAMXfrOmoGGl"
      },
      "source": [
        "from tensorflow.keras.applications.inception_v3  import InceptionV3\n",
        "model_InceptionV3 = InceptionV3(weights='imagenet') #,include_top=False\n",
        "#model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9JzCSdKoGGm"
      },
      "source": [
        "Create Feautre Arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-khQKd4oGGm"
      },
      "source": [
        "TrainFeaturesArr_InceptionV3 = model_InceptionV3.predict(train_images_InceptionV3)\n",
        "ValidateFeaturesArr_InceptionV3 = model_InceptionV3.predict(validate_images_InceptionV3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWi58C9voGGn"
      },
      "source": [
        "Add KNN layer with K = 3 and P = 1 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ct_Zb68foGGo"
      },
      "source": [
        "from sklearn import neighbors\n",
        "knn_InceptionV3 = neighbors.KNeighborsClassifier(n_neighbors = 3, p = 1)\n",
        "knn_InceptionV3.fit(TrainFeaturesArr_InceptionV3, train_labels_InceptionV3)\n",
        "y_pred_InceptionV3 = knn_InceptionV3.predict(ValidateFeaturesArr_InceptionV3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySsmlG32oGGo",
        "outputId": "88ebae68-cbdc-4211-e40f-dc39f211859a"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "print('InceptionV3 Accuracy score: ',accuracy_score(y_pred_InceptionV3, validate_labels_InceptionV3))\n",
        "print('InceptionV3 F1 Score: ',f1_score(y_pred_InceptionV3, validate_labels_InceptionV3, average=\"macro\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "InceptionV3 Accuracy score:  0.975\n",
            "InceptionV3 F1 Score:  0.9891304347826086\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "329Kwy_Z4PD0"
      },
      "source": [
        "#featuresArr = []\n",
        "#for x in train_images:\n",
        "#  features = model.predict(x)\n",
        "#  features_reduce =  features.squeeze()\n",
        "#  featuresArr.append(features_reduce)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Onmn4Iosint_"
      },
      "source": [
        "#from tensorflow.keras.applications.vgg16 import decode_predictions\n",
        "\n",
        "#urllib.request.urlretrieve(\"https://user-images.githubusercontent.com/26264000/81228345-f167f600-8fbb-11ea-8722-d25dcda78a0c.jpg\", \"sample.jpg\")\n",
        "#img_path = 'sample.jpg'\n",
        "#img = image.load_img(img_path, target_size=(224, 224))\n",
        "#x = image.img_to_array(img)\n",
        "#x = np.expand_dims(x, axis=0)\n",
        "#x = preprocess_input(x)\n",
        "\n",
        "#features = model.predict(x)\n",
        "#print('Predicted:', decode_predictions(features, top=3)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4Np8gn7gf3_"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox9XYR35gbww"
      },
      "source": [
        ""
      ]
    }
  ]
}